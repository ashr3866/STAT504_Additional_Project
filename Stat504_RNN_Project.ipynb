{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Recurrent Neural Networks for Predicting Missing Words\n",
    ">### Mohammad Ashrafuzzaman\n",
    "\n",
    ">#### STAT 504: Analytics\n",
    ">#### Professor Stephen Lee\n",
    ">#### July 01, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### TASK DESCRIPTION: \n",
    "Find a paper on the problem of predicting missing word in a sentence using Recurrent Neural Networks and reproduce the work using Python or any other suitable tool.\n",
    "\n",
    "### Outline:\n",
    "We follow the work done in the paper below. \n",
    " \n",
    " \n",
    ">Arathi Mani, Solving Text Imputation Using Recurrent Neural Networks, Course Project, Department of Computer Science, Stanford University, 2015. (Available at: https://cs224d.stanford.edu/reports/ManiArathi.pdf)\n",
    "\n",
    " \n",
    "The study reported in the paper by Mani is based on The Billion Word Imputation (https://www.kaggle.com/c/billion-word-imputation), a Kaggle challenge that provided a corpus of sentences consisting of a billion words. The author of the paper pruned the large dataset to a corpus of 10,000 sentences as they found that training the original dataset would take about 2 weeks to train. \n",
    "\n",
    "In the current work by me, I used a corpus dataset comprising of 10,000 sentences; 8000 of those are used for training, 1000 for validation and 1000 for testing. The details about this dataset is given below. \n",
    "\n",
    "Mani used the following three RNN models to train to predict likelihood of possible sentences with a new word inserted in the missing corpus. \n",
    " \n",
    "+ Vanilla Recurrent Neural Network with one hidden layer of size 100.\n",
    "+ Deep (2-layer) Recurrent Neural Network with two hidden layers of size 100.\n",
    "+ Bidirectional Recurrent Neural Network\n",
    "\n",
    "I used these three models and further qualified them with the use of Dropouts. The details about the models I used are given below.\n",
    "\n",
    "The two metrics used by Mani to measure the degree of success are: \n",
    "+ Perplexity \n",
    "+ Levenshtein distance\n",
    "\n",
    "In my study I used only perplexity. The reason for not using Levenshtein distance is described below.\n",
    "\n",
    "The evaluation results will be presented in tabular forms as well as in graphs.\n",
    "\n",
    "For programming, I used Python and the TensorFlow machine learning research library. The Python code for this experiment as an iPython Notebook, as well as the data files are uploaded to GitHub and are available at: https://github.com/ashr3866/STAT504_Additional_Project\n",
    "\n",
    "The code and the report are below (in an interleaved manner).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract:\n",
    "This exercise implements recurrent neural networks (RNN) based solution for predicting exactly one missing word in a sentence and determining the correct location of the word in the sentence. The goal of this exercise is to compare performances of different flavors of RNNs. We train three flavors of Recurrent Neural Network Language Models (RNNLM), with and without Dropout, on a large corpus of English sentences and use the trained model to predict the likelihood of missing words. We find that the bidirectional RNN with Droput has the best performance obtaining an overall perplexity score of 102.556."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "\n",
    "Basic multi-layered neural networks work well as classifiers tagging the input as belonging to one of the many classes. They are trained using the existing backpropagation algorithms. However, they are not capable of handling inputs which come in a sequence. On the other hand, Recurrent Neural Networks (RNN) make use sequential information, in the sense that they don't assume and don't treat inputs (and outputs) as independent of each other. The 'recurrent' keyword in RNNs comes from the fact that they perform the same task for every element of a sequence, however, with the output being depended on the previous computations. Another way to think about RNNs is that they have a “memory” which captures information about what has been calculated so far. RNN has been quite successfully used to implement statistical language modeling as sentences are actually sequences of words. \n",
    "\n",
    "Statistical language modeling has been shown as a crucial component of many intersting problems such as automatic speech recognition (ASR), machine translation (MT), image captioning and prediction for mobile phone text input.\n",
    "\n",
    "In this exercise we train three flavors of RNNs with and without Dropouts and compare their performances in measures of perplexity using the Penn Tree Bank (PTB) dataset, which is a popular benchmark for measuring quality of these models, whilst being small and relatively fast to train.\n",
    "\n",
    "We found that for all three models the performances increase by about 8-10% when using 'Dropouts'. In overall, the bidirectional RNN with dropouts perform the best with a perplexity of 102.556.\n",
    "\n",
    "This exercise can be used as an effective launching pad for further experimentations as the program is written in a way to expand the depth and breadth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions for reading data files and preprocessing the data\n",
    "\n",
    "The functions in the following cells are used to read the data files. The preprocessing of the data includes:\n",
    "\n",
    "+ Reading the data from files, putting each sentence as individual items in a list.\n",
    "+ Craeting lists od unique words.\n",
    "+ Assigning unique numeric IDs to each words.\n",
    "+ Replacing words in sentences with corresponding IDs, so that each sentence now consists of numeric IDs. This is done to speed up the time for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility functions for reading files\n",
    "\n",
    "def data_read_words(filename):\n",
    "  with tf.gfile.GFile(filename, \"r\") as f:\n",
    "    return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "\n",
    "def data_build_vocab(filename):\n",
    "  data = data_read_words(filename)\n",
    "\n",
    "  counter = collections.Counter(data)\n",
    "  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "  words, _ = list(zip(*count_pairs))\n",
    "  word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "  return word_to_id\n",
    "\n",
    "\n",
    "def data_file_to_word_ids(filename, word_to_id):\n",
    "  data = data_read_words(filename)\n",
    "  return [word_to_id[word] for word in data]\n",
    "\n",
    "\n",
    "def load_raw_data(data_path=None):\n",
    "  \"\"\"Load raw data from data directory \"data_path\".\n",
    "\n",
    "  Reads text files, converts strings to integer ids,\n",
    "  and performs mini-batching of the inputs.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  train_path = os.path.join(data_path, \"data.train.txt\") \n",
    "  valid_path = os.path.join(data_path, \"data.valid.txt\")\n",
    "  test_path = os.path.join(data_path, \"data.test.txt\")\n",
    "\n",
    "  word_to_id = data_build_vocab(train_path)\n",
    "  train_data = data_file_to_word_ids(train_path, word_to_id)\n",
    "  valid_data = data_file_to_word_ids(valid_path, word_to_id)\n",
    "  test_data = data_file_to_word_ids(test_path, word_to_id)\n",
    "  vocabulary = len(word_to_id)\n",
    "  return train_data, valid_data, test_data, vocabulary\n",
    "\n",
    "\n",
    "def data_iterator(raw_data, batch_size, num_steps):\n",
    "  \"\"\"Iterate on the raw data.\n",
    "\n",
    "  This generates batch_size pointers into the raw PTB data, and allows\n",
    "  minibatch iteration along these pointers.\n",
    "\n",
    "  Args:\n",
    "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "\n",
    "  Yields:\n",
    "    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n",
    "    The second element of the tuple is the same data time-shifted to the\n",
    "    right by one.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if batch_size or num_steps are too high.\n",
    "  \"\"\"\n",
    "  raw_data = np.array(raw_data, dtype=np.int32)\n",
    "\n",
    "  data_len = len(raw_data)\n",
    "  batch_len = data_len // batch_size\n",
    "  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
    "  for i in range(batch_size):\n",
    "    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
    "\n",
    "  epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "  if epoch_size == 0:\n",
    "    raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "  for i in range(epoch_size):\n",
    "    x = data[:, i*num_steps:(i+1)*num_steps]\n",
    "    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "    yield (x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modelling\n",
    "\n",
    "We use three different kinds of RNN for this experiment.\n",
    " \n",
    "+ Vanilla Recurrent Neural Network with one hidden layer of size 100.\n",
    "+ Deep (2-layer) Recurrent Neural Network with two hidden layers of size 100. Deep RNN is just a basic RNN with 2 hidden layers.\n",
    "+ Bidirectional Recurrent Neural Network of size 100. Bidirectional RNNs are based on the idea that the output at time t may not only depend on the previous elements in the sequence, but also future elements. For example, to predict a missing word in a sequence you want to look at both the left and the right context. Bidirectional RNNs are quite simple. They are just two RNNs stacked on top of each other. The output is then computed based on the hidden state of both RNNs.\n",
    "\n",
    "### LSTM\n",
    "All these three models are implemented using LSTM (Long Short Term Memory) networks. “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber in 1997. LSTMs don’t have a fundamentally different architecture from RNNs, but they use a different function to compute the hidden state. Even though in thoery, RNNs can handle very long sequences, that is not true in practice. If the sequences are quite long, the gradients (values calculated to tune the network) computed during their training (backpropagation) either vanish (multiplication of many 0 < values < 1) or explode (multiplication of many large values) causing it to train very slowly. LSTMs address this drawback. LSTMs solve the gradient problem by introducing a few more gates that control access to the cell state. \n",
    "\n",
    "### Dropout\n",
    "Another feature of the models we used is the use (or no use) of Dropout. Dropout, very recently introduced by Srivastava (in 2013) is a regularization method that has been very successful with feed-forward neural networks. Dropout involves randomly removing some hidden units in a neural network during training but keeping all of them during testing. \n",
    "\n",
    "The program is written in Python 3.5 using the TensorFlow library. TensorFlow (https://www.tensorflow.org/) is an open source software library from Google Inc. with extensive support for machine learning, including RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VanillaRNN(object):\n",
    "  num_layers = 1\n",
    "\n",
    "class TwoLayerRNN(object):\n",
    "  num_layers = 2\n",
    "\n",
    "class BidirectionalRNN(object):\n",
    "  num_layers = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations\n",
    "Each of the different RNN models can be trained using either of the following there configurations:\n",
    "\n",
    "The hyperparameters used in the configurations are:\n",
    "- init_scale - the initial scale of the weights\n",
    "- learning_rate - the initial value of the learning rate\n",
    "- max_grad_norm - the maximum permissible norm of the gradient\n",
    "- num_steps - the number of unrolled steps of LSTM\n",
    "- hidden_size - the number of LSTM units\n",
    "- max_epoch - the number of epochs trained with the initial learning rate\n",
    "- max_max_epoch - the total number of epochs for training\n",
    "- keep_prob - the probability of keeping weights in the dropout layer\n",
    "- lr_decay - the decay of the learning rate for each epoch after \"max_epoch\"\n",
    "- batch_size - the batch size\n",
    "- vocab_size - the number of words to consider\n",
    "\n",
    "\n",
    "\n",
    "| config | init_scale | learning_rate  | max_grad_norm | num_steps | hidden_size | max_epoch | max_max_epoch | keep_prob | lr_decay | batch_size|vocab_size|\n",
    "|--------|--------|-------|--------|--------| -- |-- |-- |-- |-- |-- |-- |\n",
    "| small  | 0.1    | 1.0 | 5 | 20 | 100 | 4 |13 |0.8 |0.5 |20|10000 |\n",
    "| medium | 0.05   | 1.0 | 5 | 35 | 650 | 6 |39 |0.5 |0.8 |20 |10000 |\n",
    "| large  | 0.04    | 1.0 |10 | 35 |1500 |14 |55 |0.35|0.87 |20 |10000 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class SmallConfig(object):\n",
    "  \"\"\"Small config.\"\"\"\n",
    "  init_scale = 0.1\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 5  \n",
    "  num_steps = 20\n",
    "  hidden_size = 100\n",
    "  max_epoch = 4\n",
    "  max_max_epoch = 13\n",
    "  keep_prob = 0.8\n",
    "  lr_decay = 0.5\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n",
    "\n",
    "class MediumConfig(object):\n",
    "  \"\"\"Medium config.\"\"\"\n",
    "  init_scale = 0.05\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 5\n",
    "  num_steps = 35\n",
    "  hidden_size = 650\n",
    "  max_epoch = 6\n",
    "  max_max_epoch = 39\n",
    "  keep_prob = 0.5\n",
    "  lr_decay = 0.8\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n",
    "\n",
    "class LargeConfig(object):\n",
    "  \"\"\"Large config.\"\"\"\n",
    "  init_scale = 0.04\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 10\n",
    "  num_steps = 35\n",
    "  hidden_size = 1500\n",
    "  max_epoch = 14\n",
    "  max_max_epoch = 55\n",
    "  keep_prob = 0.35\n",
    "  lr_decay = 1 / 1.15\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Testing, the following configuration is used irrespective of what configuration was used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestConfig(object):\n",
    "  \"\"\"Tiny config, for testing.\"\"\"\n",
    "  init_scale = 0.1\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 1\n",
    "  num_layers = 1\n",
    "  num_steps = 2\n",
    "  hidden_size = 2\n",
    "  max_epoch = 1\n",
    "  max_max_epoch = 1\n",
    "  keep_prob = 1.0\n",
    "  lr_decay = 0.5\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset we used for this experiment is the well-known Penn Tree Bank (PTB) dataset downloaded from https://www.cis.upenn.edu/~treebank/ web location. The PTB dataset consists of 929k training words, 73k validation words, and 82k test words. It has 10k words in its vocabulary. \n",
    "\n",
    "As stated earlier, the words are mapped to unique IDs for speeding up the processing by the RNNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_DropoutModel(object):\n",
    "  \"\"\"The LSTM_DropoutModel model.\"\"\"\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "  def __init__(self, is_training, config, model):\n",
    "#------------------------------------------------------------------------\n",
    "    self.batch_size = batch_size = config.batch_size\n",
    "    self.num_steps = num_steps = config.num_steps\n",
    "    size = config.hidden_size\n",
    "    vocab_size = config.vocab_size\n",
    "\n",
    "    self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "    with tf.device(\"/cpu:0\"): # use primary CPU; to use GPU - \"/gpu:0\"\n",
    "      embedding = tf.get_variable(\"embedding\", [vocab_size, size])\n",
    "      inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    " \n",
    "    if FLAGS_use_Dropout==\"true\":\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "    if FLAGS_model==\"BidirectionalRNN\":\n",
    "        # forward direction cell\n",
    "        lstm_cell_fw = tf.nn.rnn_cell.BasicLSTMCell(size, forget_bias=0.0)\n",
    "        # backward direction cell\n",
    "        lstm_cell_bw = tf.nn.rnn_cell.BasicLSTMCell(size, forget_bias=0.0)\n",
    "        outputs, _, _ = rnn.bidirectional_rnn(lstm_cell_fw, lstm_cell_bw, x, dtype=tf.float32)\n",
    "    else:    \n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(size, forget_bias=0.0)\n",
    "  \n",
    "    if FLAGS_use_Dropout==\"true\":\n",
    "        if is_training and config.keep_prob < 1:\n",
    "          lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob)\n",
    "\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * model.num_layers)\n",
    "    self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    outputs = []\n",
    "    state = self._initial_state\n",
    "    with tf.variable_scope(\"RNN\"):\n",
    "      for time_step in range(num_steps):\n",
    "        if time_step > 0: \n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "        outputs.append(cell_output)\n",
    "\n",
    "    output = tf.reshape(tf.concat(1, outputs), [-1, size])  \n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [size, vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    self.logits = logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    #.sequence_loss_by_example([logits], [targets], [weights]) \n",
    "    loss = tf.nn.seq2seq.sequence_loss_by_example([logits], \n",
    "                                                  [tf.reshape(self._targets, [-1])],\n",
    "                                                  [tf.ones([batch_size * num_steps])])\n",
    "    self._cost = cost = tf.reduce_sum(loss) / batch_size \n",
    "    self._final_state = state\n",
    "\n",
    "    if not is_training:\n",
    "      return\n",
    "\n",
    "    self._lr = tf.Variable(0.0, trainable=False)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n",
    "                                      config.max_grad_norm)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "    self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "#------------------------------------------------------------------------\n",
    "  def assign_lr(self, session, lr_value):\n",
    "#------------------------------------------------------------------------\n",
    "    session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "  @property\n",
    "  def input_data(self):\n",
    "    return self._input_data\n",
    "\n",
    "  @property\n",
    "  def targets(self):\n",
    "    return self._targets\n",
    "\n",
    "  @property\n",
    "  def initial_state(self):\n",
    "    return self._initial_state\n",
    "\n",
    "  @property\n",
    "  def cost(self):\n",
    "    return self._cost\n",
    "\n",
    "  @property\n",
    "  def final_state(self):\n",
    "    return self._final_state\n",
    "\n",
    "  @property\n",
    "  def lr(self):\n",
    "    return self._lr\n",
    "\n",
    "  @property\n",
    "  def train_op(self):\n",
    "    return self._train_op\n",
    "#------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "def run_epoch(session, m, data, eval_op, verbose=False):\n",
    "#------------------------------------------------------------------------\n",
    "  \"\"\"Runs the model on the given data.\"\"\"\n",
    "  epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "  start_time = time.time()\n",
    "  costs = 0.0\n",
    "  iters = 0\n",
    "  levenshtein_distance = 0.0\n",
    "\n",
    "  state = m.initial_state.eval()\n",
    "  for step, (x, y) in enumerate(data_iterator(data, m.batch_size,m.num_steps)):\n",
    "    # run the loop\n",
    "    cost, state, _ = session.run([m.cost, m.final_state, eval_op],\n",
    "                                 {m.input_data: x,\n",
    "                                  m.targets: y,\n",
    "                                  m.initial_state: state})\n",
    "    costs += cost\n",
    "    iters += m.num_steps\n",
    "\n",
    "    if verbose and step % (epoch_size // 10) == 10:\n",
    "      print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "            (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "             iters * m.batch_size / (time.time() - start_time)))\n",
    "  perplexity = np.exp(costs / iters)\n",
    "  return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "def get_config():\n",
    "#------------------------------------------------------------------------\n",
    "  if FLAGS_config == \"small\":\n",
    "    return SmallConfig()\n",
    "  elif FLAGS_config == \"medium\":\n",
    "    return MediumConfig()\n",
    "  elif FLAGS_config == \"large\":\n",
    "    return LargeConfig()\n",
    "  elif FLAGS_config == \"test\":\n",
    "    return TestConfig()\n",
    "  else:\n",
    "    raise ValueError(\"Invalid Configuration: %s\", FLAGS_config)\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def get_model():\n",
    "#------------------------------------------------------------------------\n",
    "  if FLAGS_model == \"VanillaRNN\":\n",
    "    return VanillaRNN()\n",
    "  elif FLAGS_model == \"TwoLayerRNN\":\n",
    "    return TwoLayerRNN()\n",
    "  elif FLAGS_model == \"BidirectionalRNN\":\n",
    "    return BidirectionalRNN()\n",
    "  else:\n",
    "    raise ValueError(\"%s :Model Not Supported.\", FLAGS_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "def main(_):\n",
    "#------------------------------------------------------------------------\n",
    "  starttime = datetime.datetime.now()\n",
    "  print(starttime.strftime(\"%A, %d. %B %Y %I:%M%p\"))\n",
    "  if not FLAGS_data_path:\n",
    "    raise ValueError(\"Must set --data_path to the data directory\")\n",
    "\n",
    "  raw_data = load_raw_data(FLAGS_data_path)\n",
    "  train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
    "  model = get_model()\n",
    "  config = get_config()\n",
    "  eval_config = get_config()\n",
    "  eval_config.batch_size = 1\n",
    "  eval_config.num_steps = 1\n",
    "\n",
    "  with tf.Graph().as_default(), tf.Session() as session:\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale,\n",
    "                                                config.init_scale)\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "      m = LSTM_DropoutModel(is_training=True, config=config, model=model)\n",
    "    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "      mvalid = LSTM_DropoutModel(is_training=False, config=config, model=model)\n",
    "      mtest = LSTM_DropoutModel(is_training=False, config=eval_config, model=model)\n",
    "\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for i in range(config.max_max_epoch):  # run for number of epochs\n",
    "      lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n",
    "      m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "\n",
    "      print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "      train_perplexity = run_epoch(session, m, train_data, m.train_op, verbose=True)\n",
    "      print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "      valid_perplexity = run_epoch(session, mvalid, valid_data, tf.no_op())\n",
    "      print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "\n",
    "    test_perplexity = run_epoch(session, mtest, test_data, tf.no_op())\n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "    endtime = datetime.datetime.now()\n",
    "    print(endtime.strftime(\"%A, %d. %B %Y %I:%M%p\"))\n",
    "    elapsedTime = endtime - starttime\n",
    "    print (elapsedTime)\n",
    "\n",
    "    print(\"Completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friday, 01. July 2016 01:33PM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x10fdd56d8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x10fdd56d8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x10fdd56d8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Learning rate: 1.000\n",
      "0.004 perplexity: 4464.399 speed: 1635 wps\n",
      "0.104 perplexity: 883.480 speed: 1815 wps\n",
      "0.204 perplexity: 684.727 speed: 1813 wps\n",
      "0.304 perplexity: 571.211 speed: 1770 wps\n",
      "0.404 perplexity: 500.451 speed: 1775 wps\n",
      "0.504 perplexity: 453.476 speed: 1785 wps\n",
      "0.604 perplexity: 412.215 speed: 1790 wps\n",
      "0.703 perplexity: 383.742 speed: 1795 wps\n",
      "0.803 perplexity: 361.297 speed: 1799 wps\n",
      "0.903 perplexity: 341.151 speed: 1802 wps\n",
      "Epoch: 1 Train Perplexity: 325.673\n",
      "Epoch: 1 Valid Perplexity: 204.750\n",
      "Epoch: 2 Learning rate: 1.000\n",
      "0.004 perplexity: 263.177 speed: 1815 wps\n",
      "0.104 perplexity: 197.835 speed: 1842 wps\n",
      "0.204 perplexity: 205.847 speed: 1844 wps\n",
      "0.304 perplexity: 201.774 speed: 1845 wps\n",
      "0.404 perplexity: 199.584 speed: 1846 wps\n",
      "0.504 perplexity: 198.031 speed: 1846 wps\n",
      "0.604 perplexity: 193.426 speed: 1846 wps\n",
      "0.703 perplexity: 191.275 speed: 1845 wps\n",
      "0.803 perplexity: 189.212 speed: 1845 wps\n",
      "0.903 perplexity: 185.801 speed: 1831 wps\n",
      "Epoch: 2 Train Perplexity: 183.677\n",
      "Epoch: 2 Valid Perplexity: 168.462\n",
      "Epoch: 3 Learning rate: 1.000\n",
      "0.004 perplexity: 211.615 speed: 1799 wps\n",
      "0.104 perplexity: 156.737 speed: 1838 wps\n",
      "0.204 perplexity: 166.265 speed: 1840 wps\n",
      "0.304 perplexity: 164.095 speed: 1840 wps\n",
      "0.404 perplexity: 163.718 speed: 1840 wps\n",
      "0.504 perplexity: 163.640 speed: 1839 wps\n",
      "0.604 perplexity: 160.745 speed: 1841 wps\n",
      "0.703 perplexity: 160.151 speed: 1841 wps\n",
      "0.803 perplexity: 159.530 speed: 1842 wps\n",
      "0.903 perplexity: 157.322 speed: 1826 wps\n",
      "Epoch: 3 Train Perplexity: 156.345\n",
      "Epoch: 3 Valid Perplexity: 154.374\n",
      "Epoch: 4 Learning rate: 1.000\n",
      "0.004 perplexity: 189.016 speed: 1548 wps\n",
      "0.104 perplexity: 140.234 speed: 1729 wps\n",
      "0.204 perplexity: 150.250 speed: 1706 wps\n",
      "0.304 perplexity: 148.331 speed: 1709 wps\n",
      "0.404 perplexity: 148.300 speed: 1696 wps\n",
      "0.504 perplexity: 148.716 speed: 1697 wps\n",
      "0.604 perplexity: 146.510 speed: 1677 wps\n",
      "0.703 perplexity: 146.365 speed: 1686 wps\n",
      "0.803 perplexity: 146.243 speed: 1703 wps\n",
      "0.903 perplexity: 144.454 speed: 1717 wps\n",
      "Epoch: 4 Train Perplexity: 143.861\n",
      "Epoch: 4 Valid Perplexity: 149.003\n",
      "Epoch: 5 Learning rate: 1.000\n",
      "0.004 perplexity: 176.745 speed: 1521 wps\n",
      "0.104 perplexity: 130.721 speed: 1668 wps\n",
      "0.204 perplexity: 140.699 speed: 1702 wps\n",
      "0.304 perplexity: 139.042 speed: 1700 wps\n",
      "0.404 perplexity: 139.359 speed: 1700 wps\n",
      "0.504 perplexity: 139.753 speed: 1720 wps\n",
      "0.604 perplexity: 137.860 speed: 1735 wps\n",
      "0.703 perplexity: 137.916 speed: 1748 wps\n",
      "0.803 perplexity: 138.003 speed: 1757 wps\n",
      "0.903 perplexity: 136.446 speed: 1764 wps\n",
      "Epoch: 5 Train Perplexity: 136.107\n",
      "Epoch: 5 Valid Perplexity: 142.805\n",
      "Epoch: 6 Learning rate: 0.500\n",
      "0.004 perplexity: 167.892 speed: 1818 wps\n",
      "0.104 perplexity: 122.078 speed: 1710 wps\n",
      "0.204 perplexity: 129.819 speed: 1694 wps\n",
      "0.304 perplexity: 127.062 speed: 1735 wps\n",
      "0.404 perplexity: 126.230 speed: 1759 wps\n",
      "0.504 perplexity: 126.071 speed: 1775 wps\n",
      "0.604 perplexity: 123.531 speed: 1785 wps\n",
      "0.703 perplexity: 123.075 speed: 1792 wps\n",
      "0.803 perplexity: 122.570 speed: 1799 wps\n",
      "0.903 perplexity: 120.627 speed: 1805 wps\n",
      "Epoch: 6 Train Perplexity: 119.764\n",
      "Epoch: 6 Valid Perplexity: 128.591\n",
      "Epoch: 7 Learning rate: 0.250\n",
      "0.004 perplexity: 149.553 speed: 1823 wps\n",
      "0.104 perplexity: 110.209 speed: 1850 wps\n",
      "0.204 perplexity: 118.431 speed: 1850 wps\n",
      "0.304 perplexity: 115.949 speed: 1852 wps\n",
      "0.404 perplexity: 115.477 speed: 1851 wps\n",
      "0.504 perplexity: 115.377 speed: 1850 wps\n",
      "0.604 perplexity: 113.077 speed: 1851 wps\n",
      "0.703 perplexity: 112.686 speed: 1851 wps\n",
      "0.803 perplexity: 112.281 speed: 1851 wps\n",
      "0.903 perplexity: 110.370 speed: 1851 wps\n",
      "Epoch: 7 Train Perplexity: 109.616\n",
      "Epoch: 7 Valid Perplexity: 121.898\n",
      "Epoch: 8 Learning rate: 0.125\n",
      "0.004 perplexity: 141.988 speed: 1764 wps\n",
      "0.104 perplexity: 103.984 speed: 1841 wps\n",
      "0.204 perplexity: 112.193 speed: 1843 wps\n",
      "0.304 perplexity: 110.011 speed: 1846 wps\n",
      "0.404 perplexity: 109.627 speed: 1847 wps\n",
      "0.504 perplexity: 109.719 speed: 1848 wps\n",
      "0.604 perplexity: 107.597 speed: 1849 wps\n",
      "0.703 perplexity: 107.312 speed: 1849 wps\n",
      "0.803 perplexity: 107.018 speed: 1849 wps\n",
      "0.903 perplexity: 105.246 speed: 1849 wps\n",
      "Epoch: 8 Train Perplexity: 104.531\n",
      "Epoch: 8 Valid Perplexity: 119.085\n",
      "Epoch: 9 Learning rate: 0.062\n",
      "0.004 perplexity: 136.146 speed: 1822 wps\n",
      "0.104 perplexity: 100.852 speed: 1854 wps\n",
      "0.204 perplexity: 109.145 speed: 1853 wps\n",
      "0.304 perplexity: 107.047 speed: 1852 wps\n",
      "0.404 perplexity: 106.706 speed: 1851 wps\n",
      "0.504 perplexity: 106.801 speed: 1851 wps\n",
      "0.604 perplexity: 104.765 speed: 1851 wps\n",
      "0.703 perplexity: 104.526 speed: 1850 wps\n",
      "0.803 perplexity: 104.256 speed: 1828 wps\n",
      "0.903 perplexity: 102.566 speed: 1812 wps\n",
      "Epoch: 9 Train Perplexity: 101.818\n",
      "Epoch: 9 Valid Perplexity: 117.538\n",
      "Epoch: 10 Learning rate: 0.031\n",
      "0.004 perplexity: 135.740 speed: 1680 wps\n",
      "0.104 perplexity: 99.559 speed: 1747 wps\n",
      "0.204 perplexity: 107.834 speed: 1756 wps\n",
      "0.304 perplexity: 105.603 speed: 1710 wps\n",
      "0.404 perplexity: 105.299 speed: 1738 wps\n",
      "0.504 perplexity: 105.472 speed: 1757 wps\n",
      "0.604 perplexity: 103.514 speed: 1770 wps\n",
      "0.703 perplexity: 103.222 speed: 1779 wps\n",
      "0.803 perplexity: 102.934 speed: 1785 wps\n",
      "0.903 perplexity: 101.216 speed: 1792 wps\n",
      "Epoch: 10 Train Perplexity: 100.491\n",
      "Epoch: 10 Valid Perplexity: 116.869\n",
      "Epoch: 11 Learning rate: 0.016\n",
      "0.004 perplexity: 131.016 speed: 1828 wps\n",
      "0.104 perplexity: 98.619 speed: 1851 wps\n",
      "0.204 perplexity: 106.755 speed: 1850 wps\n",
      "0.304 perplexity: 104.812 speed: 1850 wps\n",
      "0.404 perplexity: 104.526 speed: 1850 wps\n",
      "0.504 perplexity: 104.695 speed: 1850 wps\n",
      "0.604 perplexity: 102.673 speed: 1850 wps\n",
      "0.703 perplexity: 102.436 speed: 1849 wps\n",
      "0.803 perplexity: 102.186 speed: 1850 wps\n",
      "0.903 perplexity: 100.448 speed: 1850 wps\n",
      "Epoch: 11 Train Perplexity: 99.729\n",
      "Epoch: 11 Valid Perplexity: 116.509\n",
      "Epoch: 12 Learning rate: 0.008\n",
      "0.004 perplexity: 134.422 speed: 1787 wps\n",
      "0.104 perplexity: 98.403 speed: 1847 wps\n",
      "0.204 perplexity: 106.455 speed: 1848 wps\n",
      "0.304 perplexity: 104.468 speed: 1850 wps\n",
      "0.404 perplexity: 104.278 speed: 1849 wps\n",
      "0.504 perplexity: 104.415 speed: 1849 wps\n",
      "0.604 perplexity: 102.420 speed: 1827 wps\n",
      "0.703 perplexity: 102.143 speed: 1821 wps\n",
      "0.803 perplexity: 101.851 speed: 1824 wps\n",
      "0.903 perplexity: 100.152 speed: 1826 wps\n",
      "Epoch: 12 Train Perplexity: 99.434\n",
      "Epoch: 12 Valid Perplexity: 116.301\n",
      "Epoch: 13 Learning rate: 0.004\n",
      "0.004 perplexity: 132.698 speed: 1788 wps\n",
      "0.104 perplexity: 97.890 speed: 1712 wps\n",
      "0.204 perplexity: 106.124 speed: 1759 wps\n",
      "0.304 perplexity: 104.131 speed: 1741 wps\n",
      "0.404 perplexity: 103.868 speed: 1733 wps\n",
      "0.504 perplexity: 104.053 speed: 1749 wps\n",
      "0.604 perplexity: 102.132 speed: 1761 wps\n",
      "0.703 perplexity: 101.853 speed: 1770 wps\n",
      "0.803 perplexity: 101.597 speed: 1773 wps\n",
      "0.903 perplexity: 99.893 speed: 1777 wps\n",
      "Epoch: 13 Train Perplexity: 99.189\n",
      "Epoch: 13 Valid Perplexity: 116.218\n",
      "Test Perplexity: 111.254\n",
      "Friday, 01. July 2016 03:30PM\n",
      "1:57:27.598936\n",
      "Completed.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "#------------------------------------------------------------------------\n",
    "  FLAGS_data_path=\"data/\"\n",
    "  # Choose the configuration\n",
    "  # Uncomment one of the three lines below\n",
    "  FLAGS_config=\"small\" # Takes 2-3 hours to complete\n",
    "  #FLAGS_config=\"medium\"\n",
    "  #FLAGS_config=\"large\"\n",
    "\n",
    "  # Choose the Model\n",
    "  # Uncomment one of the three lines below\n",
    "  #FLAGS_model=\"VanillaRNN\" \n",
    "  #FLAGS_model=\"TwoLayerRNN\"\n",
    "  #FLAGS_model=\"BidirectionalRNN\"\n",
    "    \n",
    "  # Choose if using Dropout\n",
    "  # Uncomment one line below\n",
    "  FLAGS_use_Dropout=\"false\"\n",
    "  #FLAGS_use_Dropout=\"true\"  \n",
    "  \n",
    "  # Now train, validate and test the model\n",
    "  tf.app.run()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Output:\n",
    "The output above is for BidirectionalRNN running a small configuration without Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Analysis\n",
    "\n",
    "The work by Mani uses two different metrices for measuring the effectiveness of the RNNs: perplexity and Levenshtein distance. However, we could not apply the Levenshtein distance function (even though I had implmented the function) to this dataset as it has been preprocessed to collections of numeric IDs replacing corresponding words, and Levenshtein distance is not applicable to numbers. \n",
    "\n",
    "| RNN Model | Dropout | Perplexity|\n",
    "|-----------| --------| ----------|\n",
    "|Vanilla RNN| No      | 120.256   |\n",
    "|Vanilla RNN| Yes      |110.496 |\n",
    "|Deep RNN|No          |116.889   |\n",
    "|Deep RNN| Yes        |108.254|\n",
    "|Bidirectional RNN|No      | 111.254 |\n",
    "|Bidirectional RNN| Yes      |102.556|\n",
    "\n",
    "We implemented the Python program to be able to support three different configurations as described earlier. However, we could use only the 'small' configuration. Running the small configuration for one RNN model takes almost 2-hours to train and test. I tried to run the medium configuration once, and it was still running after 6 hours. Due to time constraints, I abandoned the idea of running the models in anything either the small configuration.\n",
    "\n",
    "As seen in the table above, and as expected, the Bidirectional RNN model performed better than the other two with the Vanilla RNN having the highest perplexity, i.e., being the worst performer.\n",
    "\n",
    "We also found that applying dropout improves the performances by about 8-10% for all three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Works\n",
    "\n",
    "The program is written very flexible way so that we can vefry easily choose a model and a particular configuration just by changing values of two program variables. Even tuning the values of hyper-parameters is comparatively easy. Therefore, this testbed can be used to perform widespread and an extensive experiment for evaluating performances for different RNNs with different configurations for language modeling. Some of the work that can be done immediately, include:\n",
    "\n",
    "1. Run the models using medium and large configurations. \n",
    "2. Add support for Levenshtein distance as a performance metric.\n",
    "3. Implement GRU (Gated Recurrent Unit) based RNNs, in place of LSTMs.\n",
    "4. Observe and plot how quickly the RNNs converge for different models.\n",
    "5. Compare our results with any such experiments done in the literature.\n",
    "6. Combine all these into a research paper and attempt to publish at a Conference or Workshop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise we trained three flavors of RNNs with and without Dropouts and compared their performances in terms of perplexity using the Penn Tree Bank (PTB) dataset.\n",
    "\n",
    "We found that for all three models the performances increase by about 8-10% when using 'Dropouts'. In overall, the bidirectional RNN with dropouts perform the best with a perplexity of 102.556.\n",
    "This exercise can be used as an effective launching pad for further experimentations as the program is written in a way to expand the depth and breadth. We just need a computing environment with better CPU/GPU strength and more memory. I think we can access the facility at CMCI, IBEST and/or NKN at the University of Idaho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
